{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from tqdm import tqdm, trange\n",
    "import time\n",
    "import torch \n",
    "from torch.optim import Adam \n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences \n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertForTokenClassification\n",
    "from transformers.tokenization_bert_japanese import BertJapaneseTokenizer\n",
    "from seqeval.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('partial_labeled_sentences.csv')\n",
    "data['compname'] = data['compname'].fillna(0)\n",
    "ddf = data.groupby('sentence').mean().reset_index()[['sentence','compname']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>word</th>\n",
       "      <th>sentence</th>\n",
       "      <th>indexloc</th>\n",
       "      <th>compname</th>\n",
       "      <th>meanval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>オリックス 銀行 と の 業務 提携 について</td>\n",
       "      <td>オリックス</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>銀行</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>と</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>の</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>業務</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24282</th>\n",
       "      <td>NaN</td>\n",
       "      <td>する</td>\n",
       "      <td>266</td>\n",
       "      <td>112</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24283</th>\n",
       "      <td>NaN</td>\n",
       "      <td>予定</td>\n",
       "      <td>266</td>\n",
       "      <td>113</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24284</th>\n",
       "      <td>NaN</td>\n",
       "      <td>です</td>\n",
       "      <td>266</td>\n",
       "      <td>114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24285</th>\n",
       "      <td>NaN</td>\n",
       "      <td>。</td>\n",
       "      <td>266</td>\n",
       "      <td>115</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24286</th>\n",
       "      <td>NaN</td>\n",
       "      <td>以上</td>\n",
       "      <td>266</td>\n",
       "      <td>116</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008547</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24287 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          text   word  sentence  indexloc  compname   meanval\n",
       "0      オリックス 銀行 と の 業務 提携 について  オリックス         1         0       1.0  0.285714\n",
       "1                          NaN     銀行         1         1       1.0  0.285714\n",
       "2                          NaN      と         1         2       0.0  0.285714\n",
       "3                          NaN      の         1         3       0.0  0.285714\n",
       "4                          NaN     業務         1         4       0.0  0.285714\n",
       "...                        ...    ...       ...       ...       ...       ...\n",
       "24282                      NaN     する       266       112       0.0  0.008547\n",
       "24283                      NaN     予定       266       113       0.0  0.008547\n",
       "24284                      NaN     です       266       114       0.0  0.008547\n",
       "24285                      NaN      。       266       115       0.0  0.008547\n",
       "24286                      NaN     以上       266       116       0.0  0.008547\n",
       "\n",
       "[24287 rows x 6 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf = ddf.rename(columns = {'compname':'meanval'})\n",
    "data = data.merge(ddf, on='sentence',how='inner')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['meanval'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data.rename(columns={'sentence':'Sentence #', 'compname':'Tag'})\n",
    "data = data.rename(columns={'word':'Word'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Tag'] = data['Tag'].replace(1,'N').replace(0,'O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "オリックス 銀行 と の 業務 提携 について\n",
      "\n",
      "20190510 当社 は オリックス 銀行 株式会社 と 、 オリックス 銀行 における 住宅 事業 者 向け サービス の 拡充 を 目的 に 顧客 紹介 業務 に関する 契約 を 締結 いたし まし た 。 本 業務 提携 を通じて 、 当社 は オリックス 銀行 より 紹介 を 受け た 住宅 事業 者 に対し 、 住宅 設備 の 延長 保証 を はじめ 定期 点検 や コール センター の 代行 など 「 人手 不足 」 に も 対応 できる 各種 サービス を 提供 し 、 オリックス 銀行 は これら を 通じ た 住宅 事業 者 向け サービス の 拡充 により 、 金融 機関 として 多様 化 する ニーズ に 対応 する 形 で 、 双方 協力 の もと 顧客 の 事業 活動 を 支援 いたし ます 。 当社 は 今後 も 住宅 事業 者 へ の ソリューション 提供 へ 真摯 に 向き合う 金融 機関 と の 提携 関係 を 強化 し 、 住宅 事業 者 の 幅広い ニーズ に お 応え する とともに 、 更 なる サービス 提供 体制 構築 を 進め て まい\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>オリックス</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>銀行</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>と</td>\n",
       "      <td>1</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>の</td>\n",
       "      <td>1</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>業務</td>\n",
       "      <td>1</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>提携</td>\n",
       "      <td>1</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>について</td>\n",
       "      <td>1</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20190510</td>\n",
       "      <td>2</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>当社</td>\n",
       "      <td>2</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>は</td>\n",
       "      <td>2</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>オリックス</td>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>銀行</td>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>株式会社</td>\n",
       "      <td>2</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>と</td>\n",
       "      <td>2</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>、</td>\n",
       "      <td>2</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>オリックス</td>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>銀行</td>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>における</td>\n",
       "      <td>2</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>住宅</td>\n",
       "      <td>2</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>事業</td>\n",
       "      <td>2</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Word  Sentence # Tag\n",
       "0      オリックス           1   N\n",
       "1         銀行           1   N\n",
       "2          と           1   O\n",
       "3          の           1   O\n",
       "4         業務           1   O\n",
       "5         提携           1   O\n",
       "6       について           1   O\n",
       "7   20190510           2   O\n",
       "8         当社           2   O\n",
       "9          は           2   O\n",
       "10     オリックス           2   N\n",
       "11        銀行           2   N\n",
       "12      株式会社           2   O\n",
       "13         と           2   O\n",
       "14         、           2   O\n",
       "15     オリックス           2   N\n",
       "16        銀行           2   N\n",
       "17      における           2   O\n",
       "18        住宅           2   O\n",
       "19        事業           2   O"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data['text'].loc[0])\n",
    "print()\n",
    "print(data['text'].loc[7])\n",
    "data.head(20)[['Word','Sentence #','Tag']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['Sentence #'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data \n",
    "        self.empty = False \n",
    "        agg_func = lambda s: [(w,t) for w,t in zip(s['Word'].values.tolist(),\n",
    "#                                                       s['POS'].values.tolist(),\n",
    "                                                      s['Tag'].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "        \n",
    "        \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1 \n",
    "            return s\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "getter = SentenceGetter(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [' '.join([y[0] for y in x]) for x in getter.sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'オリックス 銀行 と の 業務 提携 について'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [[y[1] for y in x] for x in getter.sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['N', 'N', 'O', 'O', 'O', 'O', 'O']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0, 'N': 1}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_vals = list(set(data['Tag'].values))\n",
    "tag2idx = {t:i for i,t in enumerate(tags_vals)}\n",
    "idx2tag = {i: t for t, i in tag2idx.items()}\n",
    "tag2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 200\n",
    "bs = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "n_gpu = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertJapaneseTokenizer.from_pretrained('bert-base-japanese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['オリックス', '銀行', 'と', 'の', '業務', '提携', 'について']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                         maxlen=MAX_LEN, dtype='long', truncating='post', padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12036,  2269,    13,     5,  2363,  4802,   362,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n",
    "                    maxlen=MAX_LEN, value=tag2idx['O'], padding='post',\n",
    "                    dtype='long', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_masks = [[float(i>0) for i in ii] for ii in input_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tr_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids_u, input_ids_v, tags_u, tags_v = train_test_split(input_ids,\n",
    "#                                                            tags,\n",
    "#                                                            random_state=2018,\n",
    "#                                                            test_size=0.015)\n",
    "\n",
    "# attention_masks_u, attention_masks_v, _, _ = train_test_split(attention_masks, input_ids,\n",
    "#                                             random_state=2018, test_size=0.015)\n",
    "\n",
    "\n",
    "tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids,\n",
    "                                                           tags,\n",
    "                                                           random_state=2018,\n",
    "                                                           test_size=0.3)\n",
    "\n",
    "tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                            random_state=2018, test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['株式会社 アイセルネットワークス （ グループ ） と の 業務 提携 に関する お知らせ 令 和 元年 5 月 27 日 ダウンロード',\n",
       " 'を 運営 する ウェルカム 社 と オイシックス・ラ・ 大地 が 資本 業務 提携 …',\n",
       " '2019 年 6 月 17 日 ホールディングス 株式会社 株式会社 ソフトフロントジャパンホールディングス と ソフトフロントジャパン 、 自然 会話 プラットフォーム 「 」 に関する 業務 提携 で 基本 合意 ～ コール センター 化 による コスト 削減 と 生産 性 向上 ソリューション で 協業 ～ ホールディングス 株式会社 （ 東京 都 港 区 、 代表 取締役 社長 ： 永易 俊彦 、 以下 、 ホールディングス ） と 、 株式会社 ソフト フロント ホールディングス （ 東京 都 千代田 区 、 代表 取締役 社長 ： 野田 亨 、 以下 、 ソフト フロント ホールディングス ） の 子会社 で ある 株式会社 ソフトフロントジャパン （ 東京 都 千代田 区 、 代表 取締役 社長 ： 髙須 英司 、 以下 、 ソフトフロントジャパン ） は 、 自然 会話 プラットフォーム 「 」 に関する 業務 提携 に関して 基本 合意 し まし た ので お知らせ し ます 。 ホールディングス は グループ 会社 （ 以下 、 グループ ） 体制 により 、 金融',\n",
       " '各位 2019 年 6 月 6 日 株式会社 デュアル タップ 代表 取締役 社長 臼井 貴弘 コード 3469 ／ 東証 二 部 「 」 を 運営 する と 業務 提携 スマホ ひとつ で “ 旅 する よう に 暮らす ” 「 」 に 物件 提供 東京 23 区 の 駅 近 に 特 化 し て 不動産 開発 を 行う 株式会社 デュアル タップ （ 東京 都 品川 区 ／ 代表 者 臼井 貴弘 ） は 、 急速 な 勢い で 取り扱い 物件 数 を 増やす インド の ホスピタリティ 会社 （ オヨ ） と ヤフー 株式会社 の 合弁 会社 として 、 「 （ オヨライフ ） 」 を 運営 する 株式会社 （ 東京 都 千代田 区 ／ 勝瀬 博則 ） と 物件 提供 に 向け た 業務 提携 を 締結 いたし ます 。 デュアル タップ は 、 東京 23 区 の 駅 徒歩 10 分 以内 に 特 化 し て 平均 入居 率 98 以上 を 誇る 資産 運用 型 マンション 「 （ ジーベック ） 」 の 開発 を 行っ て おり 、 東京 23 区内 で 累計 52 棟 1756',\n",
       " '『 保険 クリニック 』 が 「 」 と 業務 提携 の お知らせ']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "data_train, data_test, _, _ = train_test_split(sentences, list(range(len(sentences))), random_state=2018, test_size=0.3)\n",
    "random.sample(data_test, 5)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_inputs = torch.tensor(tr_inputs)\n",
    "val_inputs = torch.tensor(val_inputs)\n",
    "tr_tags = torch.tensor(tr_tags)\n",
    "val_tags = torch.tensor(val_tags)\n",
    "tr_masks = torch.tensor(tr_masks)\n",
    "val_masks = torch.tensor(val_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'各 ##位 2019 年 6 月 7 日 会社 名 ジェイ ・ エス ##コム ホールディングス 株式会社 代表 者 名 代表 取締役 社長 嶺 ##井 武 ##則 ( JAS ##DA ##Q ・ コード 3 7 7 9 ) 問 ##合 ##せ 先 業務 管理 統括 本部 課長 丸山 博 ##之 ( 電話 3 - 51 ##1 ##4 - 76 ##1 ) 業務 提携 に関する お ##知 ##らせ 当社 は 、 本 ##日 開催 の 取締役 会 において 、 以下 の とおり 中国 法人 江 [UNK] [UNK] [UNK] 科 ##技 有限 公司 ( 以下 、 「 掌 [UNK] 」 と いう 。 ) と の 間 で 、 中国 市場 において 、 当社 グループ が 取扱 ##う 良質 な 日本 製 化粧 品 を 掌 [UNK] が 有する インフル ##エン ##サー ##マー ##ケティング の 手法 を 活用 し 、 両社 の 更 なる 発展 と 収益 の 拡大 を 目的 と し た 業務 提携 を 行う こと について 決議 いた ##し まし た ので 、 お ##知 ##らせ いた ##し ます 。 1 . 業務 提携 の 理由 記 当社 グループ で は 日本 国内 において 、 理 美容 室 を 中心 と し た 化粧 品 の プロ [PAD] [PAD]'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([indtoword[x][0] for x in np.array(val_inputs[0])])\n",
    "# indtoword = list(tokenizer.vocab.items())\n",
    "# indtoword[2000][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n",
    "\n",
    "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForTokenClassification.from_pretrained('bert-base-japanese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_FINETUNING = False\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias','gamma','beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate':0.01},\n",
    "        {'params':[p for n,p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "        'weight_decay_rate':0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters())\n",
    "    optimizer_grouped_parameters = [{'params':[p for n,p in param_optimizer]}]\n",
    "optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score\n",
    "\n",
    "def flat_accuracy(valid_tags, pred_tags):\n",
    "\n",
    "    \"\"\"\n",
    "    Define a flat accuracy metric to use while training the model.\n",
    "    \"\"\"\n",
    "\n",
    "    return (np.array(valid_tags) == np.array(pred_tags)).mean()\n",
    "\n",
    "def annot_confusion_matrix(valid_tags, pred_tags):\n",
    "\n",
    "    \"\"\"\n",
    "    Create an annotated confusion matrix by adding label\n",
    "    annotations and formatting to sklearn's `confusion_matrix`.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create header from unique tags\n",
    "    header = sorted(list(set(valid_tags + pred_tags)))\n",
    "\n",
    "    # Calculate the actual confusion matrix\n",
    "    matrix = confusion_matrix(valid_tags, pred_tags, labels=header)\n",
    "\n",
    "    # Final formatting touches for the string output\n",
    "    mat_formatted = [header[i] + \"\\t\" + str(row) for i, row in enumerate(matrix)]\n",
    "    content = \"\\t\" + \" \".join(header) + \"\\n\" + \"\\n\".join(mat_formatted)\n",
    "\n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting train loop\n",
      "backward pass\n",
      "backward pass\n",
      "backward pass\n",
      "Train loss: 0.5859988530476888\n",
      "Train accuracy: 0.7378427576364541\n",
      "starting validation loop\n",
      "Validation loss: 0.5473500192165375\n",
      "Validation Accuracy: 0.8728249511607833\n",
      "Classificatiotn Report:\\m            precision    recall  f1-score   support\n",
      "\n",
      "        N       0.02      0.11      0.03        91\n",
      "\n",
      "micro avg       0.02      0.11      0.03        91\n",
      "macro avg       0.02      0.11      0.03        91\n",
      "\n",
      "Confusion Matrix:\n",
      " \tN O\n",
      "N\t[27 80]\n",
      "O\t[ 757 3160]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch:  20%|██        | 1/5 [01:25<05:43, 85.77s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to models/train_checkpoint_epoch_11.tar\n",
      "starting train loop\n",
      "backward pass\n",
      "backward pass\n",
      "backward pass\n",
      "Train loss: 0.579609215259552\n",
      "Train accuracy: 0.7497346598374471\n",
      "starting validation loop\n",
      "Validation loss: 0.5390204787254333\n",
      "Validation Accuracy: 0.8815705783471901\n",
      "Classificatiotn Report:\\m            precision    recall  f1-score   support\n",
      "\n",
      "        N       0.02      0.10      0.03        91\n",
      "\n",
      "micro avg       0.02      0.10      0.03        91\n",
      "macro avg       0.02      0.10      0.03        91\n",
      "\n",
      "Confusion Matrix:\n",
      " \tN O\n",
      "N\t[25 82]\n",
      "O\t[ 685 3232]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch:  40%|████      | 2/5 [02:51<04:16, 85.65s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to models/train_checkpoint_epoch_12.tar\n",
      "starting train loop\n",
      "backward pass\n",
      "backward pass\n",
      "backward pass\n",
      "Train loss: 0.5777453780174255\n",
      "Train accuracy: 0.7421524739270358\n",
      "starting validation loop\n",
      "Validation loss: 0.5308321714401245\n",
      "Validation Accuracy: 0.889066830221253\n",
      "Classificatiotn Report:\\m            precision    recall  f1-score   support\n",
      "\n",
      "        N       0.02      0.09      0.03        91\n",
      "\n",
      "micro avg       0.02      0.09      0.03        91\n",
      "macro avg       0.02      0.09      0.03        91\n",
      "\n",
      "Confusion Matrix:\n",
      " \tN O\n",
      "N\t[21 86]\n",
      "O\t[ 621 3296]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch:  60%|██████    | 3/5 [04:16<02:51, 85.65s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to models/train_checkpoint_epoch_13.tar\n",
      "starting train loop\n",
      "backward pass\n",
      "backward pass\n",
      "backward pass\n",
      "Train loss: 0.5643460154533386\n",
      "Train accuracy: 0.7651613528935526\n",
      "starting validation loop\n",
      "Validation loss: 0.5227641761302948\n",
      "Validation Accuracy: 0.8968129571577848\n",
      "Classificatiotn Report:\\m            precision    recall  f1-score   support\n",
      "\n",
      "        N       0.02      0.09      0.03        91\n",
      "\n",
      "micro avg       0.02      0.09      0.03        91\n",
      "macro avg       0.02      0.09      0.03        91\n",
      "\n",
      "Confusion Matrix:\n",
      " \tN O\n",
      "N\t[19 88]\n",
      "O\t[ 557 3360]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch:  80%|████████  | 4/5 [05:42<01:25, 85.71s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to models/train_checkpoint_epoch_14.tar\n",
      "starting train loop\n",
      "backward pass\n",
      "backward pass\n",
      "backward pass\n",
      "Train loss: 0.5575761993726095\n",
      "Train accuracy: 0.7765062524696917\n",
      "starting validation loop\n",
      "Validation loss: 0.5148117244243622\n",
      "Validation Accuracy: 0.9036845213756759\n",
      "Classificatiotn Report:\\m            precision    recall  f1-score   support\n",
      "\n",
      "        N       0.03      0.11      0.04        91\n",
      "\n",
      "micro avg       0.03      0.11      0.04        91\n",
      "macro avg       0.03      0.11      0.04        91\n",
      "\n",
      "Confusion Matrix:\n",
      " \tN O\n",
      "N\t[18 89]\n",
      "O\t[ 501 3416]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch: 100%|██████████| 5/5 [07:08<00:00, 85.72s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to models/train_checkpoint_epoch_15.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "max_grad_norm = 1.0\n",
    "pad_tok = tokenizer.vocab[\"[PAD]\"]\n",
    "sep_tok = tokenizer.vocab[\"[SEP]\"]\n",
    "cls_tok = tokenizer.vocab[\"[CLS]\"]\n",
    "o_lab = tag2idx['O']\n",
    "verbose = True\n",
    "# epoch = 0\n",
    "\n",
    "for _ in trange(epochs, desc='Epoch'):\n",
    "    epoch += 1\n",
    "    print('starting train loop')\n",
    "    model.train()\n",
    "    tr_loss, tr_accuracy = 0, 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    tr_preds, tr_labels = [], [] \n",
    "    \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        #add batch to gpu (cpu probably)\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        #forward pass \n",
    "        outputs = model(\n",
    "            b_input_ids,\n",
    "            token_type_ids=None,\n",
    "            attention_mask=b_input_mask,\n",
    "            labels=b_labels,\n",
    "        )\n",
    "        loss, tr_logits = outputs[:2]\n",
    "        print('backward pass')\n",
    "        #backward pass\n",
    "        loss.backward()\n",
    "        #compute train loss\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        #subset out unwanted predictions on CLS/PAD/SEP tokens\n",
    "        preds_mask = (\n",
    "            (b_input_ids != cls_tok) & (b_input_ids != pad_tok) & (b_input_ids != sep_tok)\n",
    "        )\n",
    "        \n",
    "        tr_logits = tr_logits.detach().cpu().numpy()\n",
    "        tr_label_ids = torch.masked_select(b_labels, (preds_mask == 1))\n",
    "        tr_batch_preds = np.argmax(tr_logits[preds_mask.squeeze()], axis=1)\n",
    "        tr_batch_labels = tr_label_ids.to('cpu').numpy()\n",
    "        tr_preds.extend(tr_batch_preds)\n",
    "        tr_labels.extend(tr_batch_labels)\n",
    "        \n",
    "        #compute training accuracy\n",
    "        tmp_tr_accuracy = flat_accuracy(tr_batch_labels, tr_batch_preds)\n",
    "        tr_accuracy += tmp_tr_accuracy\n",
    "        \n",
    "        #gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            parameters=model.parameters(), max_norm=max_grad_norm\n",
    "        )\n",
    "        \n",
    "        #update parameters\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "        \n",
    "    tr_loss = tr_loss / nb_tr_steps\n",
    "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
    "    \n",
    "    #print training loss and accuracy per epoch \n",
    "    print(f\"Train loss: {tr_loss}\")\n",
    "    print(f\"Train accuracy: {tr_accuracy}\")\n",
    "    \n",
    "    #validation loop \n",
    "    print('starting validation loop')\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    predictions, true_labels = [], []\n",
    "    \n",
    "    for batch in valid_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                b_input_ids,\n",
    "                token_type_ids=None,\n",
    "                attention_mask=b_input_mask,\n",
    "                labels=b_labels,\n",
    "            )\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "        \n",
    "        #subset out unwanted predictions on cls/pad/sep tokens\n",
    "        preds_mask = (\n",
    "            (b_input_ids != cls_tok) & (b_input_ids != pad_tok) & (b_input_ids != sep_tok)\n",
    "        )\n",
    "        \n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = torch.masked_select(b_labels, (preds_mask == 1))\n",
    "        val_batch_preds = np.argmax(logits[preds_mask.squeeze(1)], axis=1)\n",
    "        val_batch_labels = label_ids.to('cpu').numpy()\n",
    "        predictions.extend(val_batch_preds)\n",
    "        true_labels.extend(val_batch_labels)\n",
    "        \n",
    "        tmp_eval_accuracy = flat_accuracy(val_batch_labels, val_batch_preds)\n",
    "        \n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "        eval_accuracy += tmp_eval_accuracy \n",
    "        \n",
    "        nb_eval_examples += b_input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "        \n",
    "    #evaluate loss, acc, conf matrix and class report on devset \n",
    "    pred_tags = [idx2tag[i] for i in predictions]\n",
    "    valid_tags = [idx2tag[i] for i in true_labels]\n",
    "    cl_report = classification_report(valid_tags, pred_tags)\n",
    "    conf_mat = annot_confusion_matrix(valid_tags, pred_tags)\n",
    "    eval_loss = eval_loss / nb_eval_steps \n",
    "    eval_accuracy = eval_accuracy / nb_eval_steps \n",
    "    \n",
    "    #report metrics \n",
    "    print(f\"Validation loss: {eval_loss}\")\n",
    "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
    "    print(f\"Classificatiotn Report:\\m {cl_report}\")\n",
    "    if verbose:\n",
    "        print(f\"Confusion Matrix:\\n {conf_mat}\")\n",
    "        \n",
    "    #save model and optimizer state_dict following every epoch \n",
    "    save_path = f\"models/train_checkpoint_epoch_{epoch}.tar\"\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\":epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"train_loss\": tr_loss,\n",
    "            \"train_acc\": tr_accuracy,\n",
    "            \"eval_loss\":eval_loss,\n",
    "            \"eval_acc\": eval_accuracy,\n",
    "            \"classification_report\": cl_report,\n",
    "            \"confusion_matrix\": conf_mat,\n",
    "        },\n",
    "        save_path,\n",
    "    )\n",
    "    print(f\"Checkpoint saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-japanese'\n",
    "checkpoint = torch.load('models/train_checkpoint_epoch_10.tar')\n",
    "model_state_dict = checkpoint[\"model_state_dict\"]\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    model_name, num_labels=model_state_dict['classifier.weight'].shape[0]\n",
    ")\n",
    "model.load_state_dict(model_state_dict)\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "株式会社 オウケイウェイヴ について ※ 記載 さ れ た 商品 名 、 製品 名 は 各社 の 登録 商標 または 商標 です 。 © 2019 「 感謝 経済 」 プラットフォーム の グローバル 戦略 を 推進 この たび の 社 と の 資本 ・ 業務 提携 は 、 世界 的 に 決済 の キャッシュ レス 化 が 進展 する 中 で 、 暗号 資産 （ 仮想 通貨 ） を 決済 に 利用 できる デビット カード の 発行 ならびに 仮想 通貨 取引 所 の 運営 を 主力 事業 として サービスローンチ に 向け て 準備 を 進める 新興 企業 社 と の 協業 により 、 当社 が 推進 する 今後 、 当社 は グループ 会社 を通じて 社 と 下記 の 協業 を 推進 し ます 。 当社 は 社 の マーケティング パートナー として 、 当社 米国 子会社 （ 本社 ： アメリカ合衆国 カリフォルニア 州 サンフランシスコ ） が 北米 を 中心 と する 英語 圏 にて 展開 する デジタルグリーティングカードサービス 「 ブロック チェーン 運用 コンサルティ\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[CLS]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>株式会社</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>オウケイウェイヴ</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>について</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>※</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>ブロック</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>チェーン</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>運用</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>コンサルティ</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>[SEP]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         text pred\n",
       "0       [CLS]    O\n",
       "1        株式会社    O\n",
       "2    オウケイウェイヴ    N\n",
       "3        について    O\n",
       "4           ※    N\n",
       "..        ...  ...\n",
       "155      ブロック    N\n",
       "156      チェーン    O\n",
       "157        運用    N\n",
       "158    コンサルティ    O\n",
       "159     [SEP]    O\n",
       "\n",
       "[160 rows x 2 columns]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "    input_text = random.choice(data_test)\n",
    "    encoded_text = tokenizer.encode(input_text)\n",
    "    wordpieces = [tokenizer.decode(tok).replace(' ','') for tok in encoded_text]\n",
    "\n",
    "    input_ids = torch.tensor(encoded_text).unsqueeze(0).long()\n",
    "    labels = torch.tensor([1] * input_ids.size(1)).unsqueeze(0).long()\n",
    "    outputs = model(input_ids, labels=labels)\n",
    "    loss, scores = outputs[:2]\n",
    "    scores = scores.detach().numpy()\n",
    "\n",
    "    label_ids = np.argmax(scores, axis=2)\n",
    "    preds = [idx2tag[i] for i in label_ids[0]]\n",
    "\n",
    "    wp_preds = list(zip(wordpieces, preds))\n",
    "    toplevel_preds = [pair[1] for pair in wp_preds if '##' not in pair[0]]\n",
    "    str_rep = ' '.join([t[0] for t in wp_preds]).replace(' ##', '').split()\n",
    "\n",
    "    if len(str_rep) == len(toplevel_preds):\n",
    "        preds_final = list(zip(str_rep, toplevel_preds))\n",
    "        b_preds_df = pd.DataFrame(preds_final)\n",
    "        b_preds_df.columns = ['text','pred']\n",
    "        for tag in ['O','N']:\n",
    "            b_preds_df[f\"b_pred_{tag}\"] = np.where(\n",
    "                b_preds_df['pred'].str.contains(tag), 1, 0\n",
    "            )\n",
    "    else:\n",
    "        print('could not match up output string with preds.')\n",
    "    print(input_text)\n",
    "    b_preds_df.loc[:,'text':'pred']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tN O\n",
      "N\t[28 79]\n",
      "O\t[ 811 3106]\n"
     ]
    }
   ],
   "source": [
    "print(checkpoint['confusion_matrix'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.7457748398528008\n",
      "\tN O\n",
      "N\t[48 59]\n",
      "O\t[1614 2303]\n",
      "\n",
      "2\n",
      "0.7572690927263641\n",
      "\tN O\n",
      "N\t[47 60]\n",
      "O\t[1521 2396]\n",
      "\n",
      "3\n",
      "0.7917404933896688\n",
      "\tN O\n",
      "N\t[45 62]\n",
      "O\t[1424 2493]\n",
      "\n",
      "4\n",
      "0.8042342465131072\n",
      "\tN O\n",
      "N\t[43 64]\n",
      "O\t[1322 2595]\n",
      "\n",
      "5\n",
      "0.8144791240743265\n",
      "\tN O\n",
      "N\t[39 68]\n",
      "O\t[1236 2681]\n",
      "\n",
      "6\n",
      "0.8268479396665305\n",
      "\tN O\n",
      "N\t[38 69]\n",
      "O\t[1136 2781]\n",
      "\n",
      "7\n",
      "0.8373426922902185\n",
      "\tN O\n",
      "N\t[37 70]\n",
      "O\t[1051 2866]\n",
      "\n",
      "8\n",
      "0.848587070101313\n",
      "\tN O\n",
      "N\t[30 77]\n",
      "O\t[ 954 2963]\n",
      "\n",
      "9\n",
      "0.8564581345690792\n",
      "\tN O\n",
      "N\t[29 78]\n",
      "O\t[ 890 3027]\n",
      "\n",
      "10\n",
      "0.866203262005361\n",
      "\tN O\n",
      "N\t[28 79]\n",
      "O\t[ 811 3106]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,11):\n",
    "    checkpoint = torch.load('models/train_checkpoint_epoch_{}.tar'.format(str(i)))\n",
    "    print(i)\n",
    "    print(checkpoint['eval_acc'])\n",
    "    print(checkpoint['confusion_matrix'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
